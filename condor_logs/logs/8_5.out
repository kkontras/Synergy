Starting Job
/esat/smcdata/users/kkontras/Image_Dataset/no_backup/git/Synergy/train.py
/esat/smcdata/users/kkontras/Image_Dataset/no_backup/envs/balance_env/bin/python
Python 3.9.18
/esat/smcdata/users/kkontras/Image_Dataset/no_backup/git/Synergy
We have 1 GPU
/esat/smcdata/users/kkontras/Image_Dataset/no_backup/git/Synergy/train.py
config-./configs/CREMA_D/synergy/nov/synprom_ib.json
Executing command: accelerate launch /esat/smcdata/users/kkontras/Image_Dataset/no_backup/git/Synergy/train.py --config ./configs/CREMA_D/synergy/nov/synprom_ib.json --default_config ./configs/CREMA_D/default_config_cremad_res_syn.json --fold 0 --pre  --l 10 --tdqm_disable  --lr 0.0001 --wd 0.0001 --cls conformer
Namespace(config='./configs/CREMA_D/synergy/nov/synprom_ib.json', default_config='./configs/CREMA_D/default_config_cremad_res_syn.json', fold='0', alpha=None, tanh_mode=None, tanh_mode_beta=None, regby=None, clip=None, batch_size=None, l='10', multil=None, l_diffsq=None, lib=None, ratio_us=None, ratio_snr=None, kmepoch=None, num_samples=None, pow=None, nstep=None, contr_coeff=None, kde_coeff=None, etube=None, temperature=None, contr_type=None, shuffle_type=None, validate_with=None, transform_type=None, trasform_before=None, num_classes=None, base_alpha=None, alpha_var=None, base_beta=None, beta_var=None, optim_method=None, ilr_c=None, ilr_g=None, mmcosine_scaling=None, ending_epoch=None, load_ongoing=None, commonlayers=None, recon_weight1=None, recon_weight2=None, recon_epochstages=None, recon_ensemblestages=None, lr='0.0001', wd='0.0001', mm=None, cls='conformer', pre=True, frozen=False, tdqm_disable=True, start_over=False)
2025-11-03 14:13:16,219 - Config - INFO - ./configs/CREMA_D/synergy/nov/synprom_ib.json
2025-11-03 14:13:16,223 - Config - INFO -  THE Configuration of your experiment ..
{'agent': 'Agent',
 'dataset': {'data_roots': '/esat/smcdata/users/kkontras/Image_Dataset/no_backup/CremaD/CREMA-D',
             'data_split': {'fold': 0, 'method': 'non_inclusive'},
             'dataloader_class': 'CramedD_Dataloader',
             'fps': 1,
             'norm': False,
             'norm_face_path': './mydatasets/CREMAD/norm_face_path_fold{}.pkl',
             'norm_wav_path': './mydatasets/CREMAD/wav_norm_16000_fold{}.pkl',
             'num_frame': 3,
             'return_data': {'audio': False,
                             'face': False,
                             'spectrogram': True,
                             'video': True},
             'sampling_rate': 22050},
 'early_stopping': {'end_of_epoch_check': True,
                    'log_interval': 10,
                    'max_epoch': 1500,
                    'n_steps_stop': 20,
                    'n_steps_stop_after': 0,
                    'save_every_step': 3000,
                    'save_every_valstep': 10,
                    'validate_after': 0,
                    'validate_every': 25,
                    'validate_with': 'accuracy'},
 'exp_name': 'CREMAD Experiment',
 'model': {'args': {'bias_infusion': {'ending_epoch': 1500,
                                      'l': 0.1,
                                      'method': 'false',
                                      'plot': False,
                                      'starting_epoch': 0,
                                      'use': True},
                    'clip_grad': False,
                    'cls_type': 'linear',
                    'd_model': 512,
                    'dropout': 0.1,
                    'fc_inner': 64,
                    'multi_loss': {'multi_supervised_w': {'c': 0,
                                                          'combined': 1,
                                                          'g': 0}},
                    'num_classes': 6},
           'encoders': [{'args': {'d_model': 512,
                                  'dropout': 0.1,
                                  'fc_inner': 64,
                                  'freeze_encoder': False,
                                  'num_classes': 6},
                         'model': 'Audio_ResNet',
                         'pretrainedEncoder': {'dir': 'unimodal_audio_fold{}_lr0.001_wd0.0001.pth.tar',
                                               'use': False}},
                        {'args': {'d_model': 512,
                                  'dropout': 0.1,
                                  'fc_inner': 64,
                                  'freeze_encoder': False,
                                  'num_classes': 6},
                         'model': 'Video_ResNet',
                         'pretrainedEncoder': {'dir': 'unimodal_video_fold{}_lr0.001_wd0.0001.pth.tar',
                                               'use': False}}],
           'load_ongoing': False,
           'model_class': 'Fusion_Synprom_IB',
           'save_base_dir': '/esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy',
           'save_dir': 'Synprom_IB_{}.pth.tar'},
 'optimizer': {'beta1': 0.9,
               'beta2': 0.999,
               'learning_rate': 1e-05,
               'momentum': 0.9,
               'type': 'Adam',
               'weight_decay': 1e-05},
 'scheduler': {'max_lr': 0.03, 'type': 'cosanneal', 'warm_up_steps': 780},
 'training_params': {'adversarial_training': {'adv_epsilon': 0.01,
                                              'use': False},
                     'async_loading': True,
                     'batch_size': 32,
                     'cuda': True,
                     'data_loader_workers': 8,
                     'gpu_device': [0],
                     'pin_memory': False,
                     'rec_test': True,
                     'res': True,
                     'seed': 109,
                     'tdqm_disable': False,
                     'test_batch_size': 32,
                     'test_on_bottoms': False,
                     'use_test_set': True,
                     'validation': True,
                     'verbose': True,
                     'wandb_disable': False}}
2025-11-03 14:13:16,224 - Config - INFO -  *************************************** 
2025-11-03 14:13:16,224 - Config - INFO - The experiment name is CREMAD Experiment
2025-11-03 14:13:16,224 - Config - INFO -  *************************************** 
2025-11-03 14:13:16,224 - root - INFO - save_dir: Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 14:13:16,408 - Cuda Statistics - INFO - __Python VERSION:  3.9.18 (main, Sep 11 2023, 13:41:44) 
[GCC 11.2.0]
2025-11-03 14:13:16,409 - Cuda Statistics - INFO - __pyTorch VERSION:  2.0.1+cu118
2025-11-03 14:13:16,409 - Cuda Statistics - INFO - __CUDA VERSION
2025-11-03 14:13:16,410 - Cuda Statistics - INFO - __CUDNN VERSION:  8700
2025-11-03 14:13:16,436 - Cuda Statistics - INFO - __Number CUDA Devices:  1
2025-11-03 14:13:16,436 - Cuda Statistics - INFO - __Devices
2025-11-03 14:13:16,436 - Cuda Statistics - INFO - index, name, driver_version, memory.total [MiB], memory.used [MiB], memory.free [MiB]
2025-11-03 14:13:16,436 - Cuda Statistics - INFO - 0, NVIDIA GeForce RTX 3090, 580.95.05, 24576 MiB, 4 MiB, 24121 MiB
2025-11-03 14:13:16,442 - Cuda Statistics - INFO - Active CUDA Device: GPU 0
2025-11-03 14:13:16,442 - Cuda Statistics - INFO - Available devices  1
2025-11-03 14:13:16,442 - Cuda Statistics - INFO - Current cuda device  0
This path does not exist /esat/smcdata/users/kkontras/Image_Dataset/no_backup/CremaD/CREMA-D/AudioWAV/1076_MTI_SAD_XX.wav or /esat/smcdata/users/kkontras/Image_Dataset/no_backup/CremaD/CREMA-D/Image-01-FPS/1076_MTI_SAD_XX or /esat/smcdata/users/kkontras/Image_Dataset/no_backup/CremaD/CREMA-D/Face_features/1076_MTI_SAD_XX.npy
2025-11-03 14:13:18,883 - root - INFO - Loaded wav norm from ./mydatasets/CREMAD/wav_norm_16000_fold0.pkl
2025-11-03 14:13:18,886 - root - INFO - Norm values are {'mean': tensor(-2.7937e-05), 'std': tensor(0.0613), 'max_duration': 4.8715625}
2025-11-03 14:13:19,406 - root - INFO - Loaded wav norm from ./mydatasets/CREMAD/wav_norm_16000_fold0.pkl
2025-11-03 14:13:19,407 - root - INFO - Norm values are {'mean': tensor(-2.7937e-05), 'std': tensor(0.0613), 'max_duration': 4.8715625}
2025-11-03 14:13:19,954 - root - INFO - Loaded wav norm from ./mydatasets/CREMAD/wav_norm_16000_fold0.pkl
2025-11-03 14:13:19,955 - root - INFO - Norm values are {'mean': tensor(-2.7937e-05), 'std': tensor(0.0613), 'max_duration': 4.8715625}
Available cores 8
We are changing dataloader workers to num of cores 0
2025-11-03 14:13:19,957 - Agent - INFO - Device: cuda:0
2025-11-03 14:13:19,957 - Agent - INFO - Total training batches: 127, validate every 25 batches, steps per epoch: 6
2025-11-03 14:13:24,387 - Agent - INFO - Loss Weights are {'total': 1, 'combined': 1, 'c': 0, 'g': 0}
2025-11-03 14:13:24,821 - root - INFO - Loading enc best model state dict from /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/unimodal_audio_fold0_lr0.001_wd0.0001.pth.tar
Replacing module
2025-11-03 14:13:25,104 - root - INFO - Loading enc best model state dict from /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/unimodal_video_fold0_lr0.001_wd0.0001.pth.tar
Replacing module
2025-11-03 14:13:25,635 - Agent - INFO - Total number of trainable parameters are: 63218008
2025-11-03 14:13:26,085 - Agent - INFO - Loading checkpoint: /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 14:13:26,106 - Agent - ERROR - Error in loading the model: Error(s) in loading state_dict for Fusion_Synprom_IB:
	Missing key(s) in state_dict: "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.module.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.layer_norm.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.layer_norm.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.u_bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.v_bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.module.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.5.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.5.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.module.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.module.sequential.4.linear.bias". 
	Unexpected key(s) in state_dict: "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.0.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.0.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.0.sequential.3.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.1.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.1.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.1.sequential.3.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.2.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.2.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.2.sequential.3.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.3.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.3.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.3.sequential.3.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.4.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.4.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.4.sequential.3.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.0.sequential.4.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.positional_encoding.pe", "common_fc_1.common_net.encoder.layers.5.sequential.1.layer_norm.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.layer_norm.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.u_bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.v_bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.query_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.query_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.key_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.key_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.value_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.value_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.pos_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.out_proj.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.1.attention.out_proj.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.2.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.2.conv.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.4.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.5.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.5.bias", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.5.running_mean", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.5.running_var", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.5.num_batches_tracked", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.7.conv.weight", "common_fc_1.common_net.encoder.layers.5.sequential.2.sequential.7.conv.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.0.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.0.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.1.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.1.linear.bias", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.4.linear.weight", "common_fc_1.common_net.encoder.layers.5.sequential.3.sequential.4.linear.bias". 
2025-11-03 14:13:26,186 - Agent - INFO - Total number of trainable parameters are: 63218008
2025-11-03 14:17:50,794 - Agent - INFO - [37mVal Epoch 1 step 1 with [31mce_loss_combined : 1.784638 [31mtotal : 1.784638 [94mAcc_combined: 58.88 [94mAcc_c: 60.36 [94mAcc_g: 56.83 [95mCEU_cue_audio: 0.5525 [95mCEU_cue_video: 0.4423 [95mCEU_synergy: 0.1721 [95mCEU_coexistence: 0.8119 
2025-11-03 14:17:50,809 - Agent - INFO - [37mTest Epoch 1 step 1 with [31mce_loss_combined : 1.784431 [31mtotal : 1.784431 [94mAcc_combined: 59.07 [94mAcc_c: 53.73 [94mAcc_g: 54.28 
2025-11-03 14:17:52,697 - Agent - INFO - [37mModels has saved successfully in /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 14:22:14,161 - Agent - INFO - [37mVal Epoch 2 step 2 with [31mce_loss_combined : 1.773238 [31mtotal : 1.773238 [94mAcc_combined: 75.97 [94mAcc_c: 62.19 [94mAcc_g: 56.61 [95mCEU_cue_audio: 0.6668 [95mCEU_cue_video: 0.7759 [95mCEU_synergy: 0.2871 [95mCEU_coexistence: 0.9625 
2025-11-03 14:22:14,175 - Agent - INFO - [37mTest Epoch 2 step 2 with [31mce_loss_combined : 1.773005 [31mtotal : 1.773005 [94mAcc_combined: 67.63 [94mAcc_c: 54.95 [94mAcc_g: 53.17 
2025-11-03 14:22:16,079 - Agent - INFO - [37mModels has saved successfully in /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 14:26:37,569 - Agent - INFO - [37mVal Epoch 3 step 3 with [31mce_loss_combined : 1.767250 [31mtotal : 1.767250 [94mAcc_combined: 77.56 [94mAcc_c: 63.90 [94mAcc_g: 55.01 [95mCEU_cue_audio: 0.6712 [95mCEU_cue_video: 0.8181 [95mCEU_synergy: 0.3360 [95mCEU_coexistence: 0.9625 
2025-11-03 14:26:37,583 - Agent - INFO - [37mTest Epoch 3 step 3 with [31mce_loss_combined : 1.768397 [31mtotal : 1.768397 [94mAcc_combined: 71.86 [94mAcc_c: 56.28 [94mAcc_g: 55.06 
2025-11-03 14:26:39,557 - Agent - INFO - [37mModels has saved successfully in /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 14:31:02,581 - Agent - INFO - [37mVal Epoch 4 step 4 with [31mce_loss_combined : 1.765287 [31mtotal : 1.765287 [94mAcc_combined: 71.18 [94mAcc_c: 62.76 [94mAcc_g: 56.26 [95mCEU_cue_audio: 0.5204 [95mCEU_cue_video: 0.7878 [95mCEU_synergy: 0.3691 [95mCEU_coexistence: 0.9032 
2025-11-03 14:35:24,287 - Agent - INFO - [37mVal Epoch 5 step 5 with [31mce_loss_combined : 1.763366 [31mtotal : 1.763366 [94mAcc_combined: 73.01 [94mAcc_c: 59.57 [94mAcc_g: 57.97 [95mCEU_cue_audio: 0.5024 [95mCEU_cue_video: 0.8484 [95mCEU_synergy: 0.2952 [95mCEU_coexistence: 0.9542 
2025-11-03 14:39:59,262 - Agent - INFO - [37mVal Epoch 6 step 6 with [31mce_loss_combined : 1.763931 [31mtotal : 1.763931 [94mAcc_combined: 72.44 [94mAcc_c: 60.71 [94mAcc_g: 58.54 [95mCEU_cue_audio: 0.4657 [95mCEU_cue_video: 0.8728 [95mCEU_synergy: 0.2460 [95mCEU_coexistence: 0.9677 
2025-11-03 14:44:22,980 - Agent - INFO - [37mVal Epoch 7 step 7 with [31mce_loss_combined : 1.763399 [31mtotal : 1.763399 [94mAcc_combined: 71.75 [94mAcc_c: 62.07 [94mAcc_g: 57.63 [95mCEU_cue_audio: 0.4293 [95mCEU_cue_video: 0.8484 [95mCEU_synergy: 0.3441 [95mCEU_coexistence: 0.9516 
2025-11-03 14:48:51,364 - Agent - INFO - [37mVal Epoch 8 step 8 with [31mce_loss_combined : 1.764452 [31mtotal : 1.764452 [94mAcc_combined: 69.36 [94mAcc_c: 62.87 [94mAcc_g: 57.29 [95mCEU_cue_audio: 0.4064 [95mCEU_cue_video: 0.8362 [95mCEU_synergy: 0.3441 [95mCEU_coexistence: 0.9141 
2025-11-03 14:53:28,925 - Agent - INFO - [37mVal Epoch 9 step 9 with [31mce_loss_combined : 1.762737 [31mtotal : 1.762737 [94mAcc_combined: 71.41 [94mAcc_c: 58.88 [94mAcc_g: 56.95 [95mCEU_cue_audio: 0.4840 [95mCEU_cue_video: 0.8302 [95mCEU_synergy: 0.2705 [95mCEU_coexistence: 0.9436 
2025-11-03 14:57:53,561 - Agent - INFO - [37mVal Epoch 10 step 10 with [31mce_loss_combined : 1.761668 [31mtotal : 1.761668 [94mAcc_combined: 70.50 [94mAcc_c: 59.91 [94mAcc_g: 57.06 [95mCEU_cue_audio: 0.4246 [95mCEU_cue_video: 0.8606 [95mCEU_synergy: 0.2621 [95mCEU_coexistence: 0.9462 
2025-11-03 15:02:20,708 - Agent - INFO - [37mVal Epoch 11 step 11 with [31mce_loss_combined : 1.764355 [31mtotal : 1.764355 [94mAcc_combined: 69.48 [94mAcc_c: 58.09 [94mAcc_g: 54.44 [95mCEU_cue_audio: 0.3925 [95mCEU_cue_video: 0.8547 [95mCEU_synergy: 0.3115 [95mCEU_coexistence: 0.9273 
2025-11-03 15:06:43,011 - Agent - INFO - [37mVal Epoch 12 step 12 with [31mce_loss_combined : 1.763103 [31mtotal : 1.763103 [94mAcc_combined: 66.86 [94mAcc_c: 58.54 [94mAcc_g: 53.87 [95mCEU_cue_audio: 0.3699 [95mCEU_cue_video: 0.8244 [95mCEU_synergy: 0.3115 [95mCEU_coexistence: 0.8924 
2025-11-03 15:11:06,830 - Agent - INFO - [37mVal Epoch 13 step 13 with [31mce_loss_combined : 1.762737 [31mtotal : 1.762737 [94mAcc_combined: 71.07 [94mAcc_c: 58.77 [94mAcc_g: 56.38 [95mCEU_cue_audio: 0.4110 [95mCEU_cue_video: 0.8547 [95mCEU_synergy: 0.3115 [95mCEU_coexistence: 0.9542 
2025-11-03 15:15:30,203 - Agent - INFO - [37mVal Epoch 14 step 14 with [31mce_loss_combined : 1.760938 [31mtotal : 1.760938 [94mAcc_combined: 71.75 [94mAcc_c: 59.23 [94mAcc_g: 54.10 [95mCEU_cue_audio: 0.4840 [95mCEU_cue_video: 0.8425 [95mCEU_synergy: 0.2871 [95mCEU_coexistence: 0.9408 
2025-11-03 15:19:53,704 - Agent - INFO - [37mVal Epoch 15 step 15 with [31mce_loss_combined : 1.761731 [31mtotal : 1.761731 [94mAcc_combined: 69.48 [94mAcc_c: 59.34 [94mAcc_g: 55.69 [95mCEU_cue_audio: 0.4338 [95mCEU_cue_video: 0.8362 [95mCEU_synergy: 0.2705 [95mCEU_coexistence: 0.9247 
2025-11-03 15:24:18,069 - Agent - INFO - [37mVal Epoch 16 step 16 with [31mce_loss_combined : 1.763101 [31mtotal : 1.763101 [94mAcc_combined: 68.45 [94mAcc_c: 59.45 [94mAcc_g: 52.16 [95mCEU_cue_audio: 0.4338 [95mCEU_cue_video: 0.7940 [95mCEU_synergy: 0.3197 [95mCEU_coexistence: 0.9032 
2025-11-03 15:28:41,181 - Agent - INFO - [37mVal Epoch 17 step 17 with [31mce_loss_combined : 1.763193 [31mtotal : 1.763193 [94mAcc_combined: 67.43 [94mAcc_c: 57.74 [94mAcc_g: 56.04 [95mCEU_cue_audio: 0.3561 [95mCEU_cue_video: 0.8425 [95mCEU_synergy: 0.3197 [95mCEU_coexistence: 0.9032 
2025-11-03 15:33:04,102 - Agent - INFO - [37mVal Epoch 18 step 18 with [31mce_loss_combined : 1.759347 [31mtotal : 1.759347 [94mAcc_combined: 71.53 [94mAcc_c: 57.63 [94mAcc_g: 57.74 [95mCEU_cue_audio: 0.4657 [95mCEU_cue_video: 0.8787 [95mCEU_synergy: 0.2541 [95mCEU_coexistence: 0.9408 
2025-11-03 15:37:28,301 - Agent - INFO - [37mVal Epoch 19 step 19 with [31mce_loss_combined : 1.763368 [31mtotal : 1.763368 [94mAcc_combined: 69.02 [94mAcc_c: 61.16 [94mAcc_g: 56.61 [95mCEU_cue_audio: 0.3789 [95mCEU_cue_video: 0.8484 [95mCEU_synergy: 0.3607 [95mCEU_coexistence: 0.9113 
2025-11-03 15:41:52,004 - Agent - INFO - [37mVal Epoch 20 step 20 with [31mce_loss_combined : 1.762979 [31mtotal : 1.762979 [94mAcc_combined: 70.50 [94mAcc_c: 54.44 [94mAcc_g: 60.02 [95mCEU_cue_audio: 0.4202 [95mCEU_cue_video: 0.8909 [95mCEU_synergy: 0.2541 [95mCEU_coexistence: 0.9382 
2025-11-03 15:46:25,865 - Agent - INFO - [37mVal Epoch 21 step 21 with [31mce_loss_combined : 1.761113 [31mtotal : 1.761113 [94mAcc_combined: 68.45 [94mAcc_c: 57.18 [94mAcc_g: 55.13 [95mCEU_cue_audio: 0.4338 [95mCEU_cue_video: 0.8181 [95mCEU_synergy: 0.3115 [95mCEU_coexistence: 0.8952 
2025-11-03 15:50:48,415 - Agent - INFO - [37mVal Epoch 22 step 22 with [31mce_loss_combined : 1.764924 [31mtotal : 1.764924 [94mAcc_combined: 65.95 [94mAcc_c: 56.15 [94mAcc_g: 54.44 [95mCEU_cue_audio: 0.3974 [95mCEU_cue_video: 0.7999 [95mCEU_synergy: 0.2952 [95mCEU_coexistence: 0.8709 
2025-11-03 15:50:49,338 - Agent - INFO - We are in the final state.
2025-11-03 15:50:50,130 - root - INFO - Loaded best model from /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
2025-11-03 15:51:07,251 - Agent - INFO - [37mVal Epoch 22 step 0 with [31mce_loss_combined : 1.767250 [31mtotal : 1.767250 [94mAcc_combined: 77.56 [94mAcc_c: 63.90 [94mAcc_g: 55.01 [95mCEU_cue_audio: 0.6712 [95mCEU_cue_video: 0.8181 [95mCEU_synergy: 0.3360 [95mCEU_coexistence: 0.9625 
2025-11-03 15:51:24,408 - Agent - INFO - [37mTest Epoch 22 step 0 with [31mce_loss_combined : 1.768397 [31mtotal : 1.768397 [94mAcc_combined: 71.86 [94mAcc_c: 56.28 [94mAcc_g: 55.06 
2025-11-03 15:51:28,910 - Agent - INFO - [37mModels has saved successfully in /esat/smcdata/users/kkontras/Image_Dataset/no_backup/data/2025_data/synergy/Synprom_IB_fold0_l10_lr0.0001_wd0.0001_clsconformer_pre.pth.tar
Job finished
